
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Point-to-Point Communication &#8212; A Python Introduction to Parallel Programming with MPI 1.0.2 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Collective Communication" href="collectiveCom.html" />
    <link rel="prev" title="Introduction to MPI" href="introMPI.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="collectiveCom.html" title="Collective Communication"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="introMPI.html" title="Introduction to MPI"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">A Python Introduction to Parallel Programming with MPI 1.0.2 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="point-to-point-communication">
<span id="pointtopoint"></span><h1>Point-to-Point Communication<a class="headerlink" href="#point-to-point-communication" title="Permalink to this headline">¶</a></h1>
<p>As mentioned in earlier, the simplest message passing involves two processes: a sender and a receiver. Let us begin by demonstrating a program designed for two processes. One will draw a random number and then send it to the other. We will do this using the routines <code class="docutils literal notranslate"><span class="pre">Comm.Send</span></code> and <code class="docutils literal notranslate"><span class="pre">Comm.Recv</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#passRandomDraw.py</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="k">import</span> <span class="n">MPI</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>

<span class="n">randNum</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">randNum</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span> <span class="s2">&quot;Process&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;drew the number&quot;</span><span class="p">,</span> <span class="n">randNum</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">comm</span><span class="o">.</span><span class="n">Send</span><span class="p">(</span><span class="n">randNum</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span> <span class="s2">&quot;Process&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;before receiving has the number&quot;</span><span class="p">,</span> <span class="n">randNum</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">comm</span><span class="o">.</span><span class="n">Recv</span><span class="p">(</span><span class="n">randNum</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span> <span class="s2">&quot;Process&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;received the number&quot;</span><span class="p">,</span> <span class="n">randNum</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>To demonstrate that the code is working the way we expect, we have the drawing process print the value that it drew. Then we have the receiving process print the value of the vairable into which we will receive to show that it is zero, and then we print the value that it receives after the call to <code class="docutils literal notranslate"><span class="pre">Recv</span></code>. We now give the syntax of the <code class="docutils literal notranslate"><span class="pre">Send</span></code> and <code class="docutils literal notranslate"><span class="pre">Recv</span></code> rountines in the following section.</p>
<div class="section" id="send-and-recv">
<h2>Send(…) and Recv(…)<a class="headerlink" href="#send-and-recv" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="Comm.Send">
<code class="descclassname">Comm.</code><code class="descname">Send</code><span class="sig-paren">(</span><em>buf</em>, <em>dest = 0</em>, <em>tag = 0</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Send" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a basic send. This send is a point-to-point communication. It sends information from exactly one process to exactly one other process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Comm</strong> (<em>MPI comm</em>) – communicator we wish to query</p></li>
<li><p><strong>buf</strong> (<em>choice</em>) – data to send</p></li>
<li><p><strong>dest</strong> (<em>integer</em>) – rank of destination</p></li>
<li><p><strong>tag</strong> (<em>integer</em>) – message tag</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="k">import</span> <span class="n">MPI</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="k">if</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span><span class="o">.</span><span class="n">Send</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dest</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
        <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span><span class="o">.</span><span class="n">Recv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">source</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<dl class="function">
<dt id="Comm.Recv">
<code class="descclassname">Comm.</code><code class="descname">Recv</code><span class="sig-paren">(</span><em>buf</em>, <em>source = 0</em>, <em>tag = 0</em>, <em>Status status = None</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Recv" title="Permalink to this definition">¶</a></dt>
<dd><p>Basic point-to-point receive of data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Comm</strong> (<em>MPI comm</em>) – communicator we wish to query</p></li>
<li><p><strong>buf</strong> (<em>choice</em>) – initial address of receive buffer (choose receipt location)</p></li>
<li><p><strong>source</strong> (<em>integer</em>) – rank of source</p></li>
<li><p><strong>tag</strong> (<em>integer</em>) – message tag</p></li>
<li><p><strong>status</strong> (<em>Status</em>) – status of object</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<blockquote>
<div><p>See example for Comm.Send</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">Send</span></code> and <code class="docutils literal notranslate"><span class="pre">Recv</span></code> are referred to as <em>blocking</em> functions. That is, if a process calls <code class="docutils literal notranslate"><span class="pre">Recv</span></code>, it will sit idle until it has received a message from a corresponding <code class="docutils literal notranslate"><span class="pre">Send</span></code> before it will proceeed. See the appendix for the corresponding <em>non-blocking</em> functions <code class="docutils literal notranslate"><span class="pre">Isend</span></code> and <code class="docutils literal notranslate"><span class="pre">Irecv</span></code> (<em>I</em> stands for immediate). In essence, <code class="docutils literal notranslate"><span class="pre">Irecv</span></code> will return immediately. If it did not receive its message it will indicate to the system that it will be receiving a message, proceed beyond the <code class="docutils literal notranslate"><span class="pre">Irecv</span></code> to do other useful work, and then check back later to see if the message has arrived. This can be used to dramatically improve performance.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>On a <code class="docutils literal notranslate"><span class="pre">Recv</span></code> you do not always need to specify the source. Instead, you can allow the calling process to accept a message from any process that happend to be sending to the receiving process. This is done by setting source to a predefined MPI constant, <code class="docutils literal notranslate"><span class="pre">source=ANY_SOURCE</span></code> (note that you would first need to import this with from <code class="docutils literal notranslate"><span class="pre">mpi4py.MPI</span> <span class="pre">import</span> <span class="pre">ANY_SOURCE</span></code> or use the syntax <code class="docutils literal notranslate"><span class="pre">source=MPI.ANY_SOURCE</span></code>).</p>
</div>
</div>
<div class="section" id="the-trapezoidal-rule">
<h2>The Trapezoidal Rule<a class="headerlink" href="#the-trapezoidal-rule" title="Permalink to this headline">¶</a></h2>
<p>Now that we understand basic communication in MPI, we will proceed by parallelizing our first algorithm–numerical integration using the “trapezoid rule.” Early on in most calculus classes, students learn to estimate integrals using the trapezoid rule. A range to be integrated is divided into many vertical slivers, and each sliver is approximated with a trapezoid. The area of each trapezoid is computed, and then all their areas are added together.</p>
<div class="math notranslate nohighlight">
\[area\approx\sum_{i=0}^{n}\frac{[f(a)+f(b)]}{2}\cdot\Delta x=\left[\frac{f(a)+f(b)}{2}+\sum_{i=0}^{n}f(a+i\Delta x)+f(a+(i+1)\Delta x)\right]\cdot\Delta x\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta x=(b-a)/n\)</span>.</p>
<p>In Python, a simple serial formulation of the trapezoidal rule would be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#trapSerial.py</span>
<span class="c1">#example to run: python trapSerial.py 0.0 1.0 10000</span>

<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c1">#takes in command-line arguments [a,b,n]</span>
<span class="n">a</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span>

<span class="k">def</span> <span class="nf">integrateRange</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Numerically integrate with the trapezoid rule on the interval from</span>
<span class="sd">        a to b with n trapezoids.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">integral</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">b</span><span class="p">))</span><span class="o">/</span><span class="mf">2.0</span>
        <span class="c1"># n+1 endpoints, but n trapazoids</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">integral</span> <span class="o">=</span> <span class="n">integral</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">integral</span> <span class="o">=</span> <span class="n">integral</span><span class="o">*</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="k">return</span> <span class="n">integral</span>

<span class="n">integral</span> <span class="o">=</span> <span class="n">integrateRange</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="nb">print</span> <span class="s2">&quot;With n =&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;trapezoids, our estimate of the integral</span><span class="se">\</span>
<span class="s2">from&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="s2">&quot;to&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="n">integral</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notice how we use <code class="docutils literal notranslate"><span class="pre">numpy.linspace</span></code>. Here <code class="docutils literal notranslate"><span class="pre">linspace</span></code> creates an iterable array that includes <em>n+1</em> evenly spaced points (among which are the endpoints). We subtract half of the value of the function evaluated at the two endpoints to accomodate the formula laid out above.</p>
</div>
<p>In this program, the bulk of the work is executed in the <code class="docutils literal notranslate"><span class="pre">integrateRange</span></code> subroutine. For values of x from a to b, the integral is iteratively summed then returned. Since the area of each trapezoid can be calculated independently of the others, this algorithm can easily be parallelized.</p>
</div>
<div class="section" id="parallelizing-the-trapezoidal-rule">
<h2>Parallelizing the Trapezoidal Rule<a class="headerlink" href="#parallelizing-the-trapezoidal-rule" title="Permalink to this headline">¶</a></h2>
<p>The most important step in parallelizing an algorithm is finding the independent computations. This is the first step of architecting a parallel algorithm. With the trapezoidal rule, it’s easy to see that the calculation of the area of trapezoid can be performed independently of any other trapezoid, and so dividing the data at the trapezoid level seems natural. The code divides up the interval into the calculation of <em>n</em> trapezoids. To prallelize this process, we will divide the interval into <em>n</em> trapeziods and then divide up those <em>n</em> trapezoids to be calculated among the number of processors, <em>size</em>.  Look at the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#trapParallel_1.py</span>
<span class="c1">#example to run: mpiexec -n 4 python trapParallel_1.py 0.0 1.0 10000</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="kn">from</span> <span class="nn">mpi4py.MPI</span> <span class="kn">import</span> <span class="n">ANY_SOURCE</span>

<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_size</span><span class="p">()</span>

<span class="c1">#takes in command-line arguments [a,b,n]</span>
<span class="n">a</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

<span class="c1">#we arbitrarily define a function to integrate</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span>

<span class="c1">#this is the serial version of the trapezoidal rule</span>
<span class="c1">#parallelization occurs by dividing the range among processes</span>
<span class="k">def</span> <span class="nf">integrateRange</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">integral</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">b</span><span class="p">))</span><span class="o">/</span><span class="mf">2.0</span>
        <span class="c1"># n+1 endpoints, but n trapazoids</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                        <span class="n">integral</span> <span class="o">=</span> <span class="n">integral</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">integral</span> <span class="o">=</span> <span class="n">integral</span><span class="o">*</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="k">return</span> <span class="n">integral</span>


<span class="c1">#h is the step size. n is the total number of trapezoids</span>
<span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
<span class="c1">#local_n is the number of trapezoids each process will calculate</span>
<span class="c1">#note that size must divide n</span>
<span class="n">local_n</span> <span class="o">=</span> <span class="n">n</span><span class="o">/</span><span class="n">size</span>

<span class="c1">#we calculate the interval that each process handles</span>
<span class="c1">#local_a is the starting point and local_b is the endpoint</span>
<span class="n">local_a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">rank</span><span class="o">*</span><span class="n">local_n</span><span class="o">*</span><span class="n">h</span>
<span class="n">local_b</span> <span class="o">=</span> <span class="n">local_a</span> <span class="o">+</span> <span class="n">local_n</span><span class="o">*</span><span class="n">h</span>

<span class="c1">#initializing variables. mpi4py requires that we pass numpy objects.</span>
<span class="n">integral</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">recv_buffer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># perform local computation. Each process integrates its own interval</span>
<span class="n">integral</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">integrateRange</span><span class="p">(</span><span class="n">local_a</span><span class="p">,</span> <span class="n">local_b</span><span class="p">,</span> <span class="n">local_n</span><span class="p">)</span>

<span class="c1"># communication</span>
<span class="c1"># root node receives results from all processes and sums them</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">integral</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
                <span class="n">comm</span><span class="o">.</span><span class="n">Recv</span><span class="p">(</span><span class="n">recv_buffer</span><span class="p">,</span> <span class="n">ANY_SOURCE</span><span class="p">)</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">recv_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
        <span class="c1"># all other process send their result</span>
        <span class="n">comm</span><span class="o">.</span><span class="n">Send</span><span class="p">(</span><span class="n">integral</span><span class="p">)</span>

<span class="c1"># root process prints results</span>
<span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;With n =&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;trapezoids, our estimate of the integral from&quot;</span>\
        <span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="s2">&quot;to&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="n">total</span>
</pre></div>
</div>
<p>The parallel approach to trapezoidal integral estimation starts by dividing the original range among the processors. Each process will get a group of one or more trapezoids to calculate area over. Now, notice how we decided to implement to division of trapezoids among the processes. The processors individually calculate their own ranges to work on. Although this is a small detail, it is fairly important. We could have written the algorithm such that process 0 divides up the work for the other processors, then each processor calculates its area, and finally a sum is computed. However, this would introduce an unnecessary bottleneck: all processes with rank greater than 0 would be waiting for its data range to arrive. By having each process calculate its own range, we gain a large speedup.</p>
<p>Once the integrals are calculated, they are summed up onto process 0. Each process with a rank higher than 0 sends it’s integral to process 0. The first parameter to the <code class="docutils literal notranslate"><span class="pre">Send</span></code> command is an array storing the information your program wishes to send.</p>
<p>At the same time, process 0 receives the data from any process. This is what the tag <code class="docutils literal notranslate"><span class="pre">ANY_SOURCE</span></code> means. It tells MPI to not worry about the sender, but rather to just accept data as it comes. When <code class="docutils literal notranslate"><span class="pre">Comm.Recv</span></code> is called, the process must wait for a message to be sent to it. If multiple processes are sending a message to the process with <code class="docutils literal notranslate"><span class="pre">Comm.Send</span></code>, the program will call <code class="docutils literal notranslate"><span class="pre">Comm.Recv</span></code> multiple times – once for each message. The for-loop essentially accomplishes this.</p>
<p>MPI has two mechanisms specifically designed to partition the message space: tags and communicators. The tag parameter is there in the case that two messages with the same size and datatype are sent to the same process. In that case, the program would not necessarily be able to tell apart the data. So the programmer can attach different tags that he or she defines to the sent data to keep them straight.</p>
<p>At this point, you should test the code for yourself. Save the code in a file named <em>trapParallel_1.py</em> and try running it from the command line using the following input:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ mpiexec -n 4 python trapParallel_1.py 0.0 1.0 10000
</pre></div>
</div>
<p>The output should appear like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">With</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span> <span class="n">trapezoids</span><span class="p">,</span> <span class="n">our</span> <span class="n">estimate</span> <span class="n">of</span> <span class="n">the</span> <span class="n">integral</span> <span class="kn">from</span> <span class="mf">0.0</span> <span class="n">to</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="mf">0.333333335</span>
</pre></div>
</div>
<p>We have successfully parallelized our first algorithm!</p>
</div>
<div class="section" id="load-balancing">
<h2>Load Balancing<a class="headerlink" href="#load-balancing" title="Permalink to this headline">¶</a></h2>
<p>Now that we have successfuly parallelized the trapezoid rule, I would like to point out a few details about what we have done. To begin, notice what happens if the number of processes does not evenly divide the number of trapezoids. Our code will break down. Try running the trapezoid program with n = 10007 trapezoids:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ mpiexec -n 4 python trapParallel_1.py 0.0 1.0 10007
</pre></div>
</div>
<p>This will produce the folowing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">With</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10007</span> <span class="n">trapezoids</span><span class="p">,</span> <span class="n">our</span> <span class="n">estimate</span> <span class="n">of</span> <span class="n">the</span> <span class="n">integral</span> <span class="kn">from</span> <span class="mf">0.0</span> <span class="n">to</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="mf">0.333033634715</span>
</pre></div>
</div>
<p>We know that the estimate of the integral whould improve as <em>n</em> grows larger. This however is much worse. This is because <code class="docutils literal notranslate"><span class="pre">local_n</span></code>, the number of trapezoids that each processor calculates, must be an integer. To solve this problem, we could require that the user always choose a value of n that is divisible by the number of processors, but good parallel code should let the user worry as little as possible about the parallelization and should function exactly as the serial version does. Thus, we could improve the code to let it handle the case where <em>n</em> is not divisible by the number of processes. How could we do this?</p>
<p>One way to solve this problem would be to designate one process to handle the remainder of the division of <code class="docutils literal notranslate"><span class="pre">n</span></code> by <code class="docutils literal notranslate"><span class="pre">size</span></code>. Give each process <em>floor(n/size)</em> trapezoids and add the remainder, <em>n modulo size</em>, to one of the processes. This would solve the problem because <code class="docutils literal notranslate"><span class="pre">local_n</span></code> would be an integer. But there a great inefficiency in doing it this way. For example, what if we ran the program with 100 processes and n=1099 trapezoids. Then each process would have <em>floor(1099/100) = 10</em> trapezoids to calculate except for the one that will also calculate the remainder, <em>1099 modulo 100 = 99</em> trapezoids. This means that one process will be calculating 109 trapezoids compared to the rest that will be calculating only 10. The program can complete be no faster than the slowest process and in this case, one process has to do over 10 times the work as the others. The processes that calculate only 10 processes will end up waiting didle until the other finishes. This is inefficient and, ignoring communication and overhead, this program could be nearly 10 times faster if we divided up the work more evenly. (See exercise 4.)</p>
<p>The important concept to remember is that any time a process is idling, waiting for another process to complete, we are losing efficiency. In this case, the work was not divided evely. We call this principle <em>load balancing.</em></p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>To conclude, notice that we have written our first parallel algorithm. We parallelized it by divided up the data. This is the most important feature in obtaining the desired speedups. We also discussed the concept of “load balancing.” A program is inefficient when the workload is not evenly divided. We discussed this fix. However, even after this fix, the algorithm is not as efficient as it could be. Look at the summation that process 0 does. While it sums the data, the other processors sit idly. Also, process zero has to handle all of the communication with the other processes one by one. The algorithm has the bottle neck of summing up all the data and communicates with each other process one by one. In the next chapter, we will introduce a technique referred to as collective communication to help fix this problem.</p>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Rewrite the first example of code “passRandomDraw.py” so that it passes instead a n x 1 vector of random draws from one process to the other. For practice, write it so that the user inputs at execution the value of n on the command-line (similar to the code developed in this section for the trapezoid rule.</p></li>
<li><p>Try modifying some of the parameters in <code class="docutils literal notranslate"><span class="pre">comm.Send</span></code> and <code class="docutils literal notranslate"><span class="pre">comm.Recv</span></code> in the code from the previous exercise (<em>dest, source, and tag</em>).  What happens to the program? Does it hang or crash? What do you suppose the <em>tag</em> parameter does?</p></li>
<li><p>We will again rewrite the first example code “passRandomDraw.py”. This time write the program so that each process <em>i</em> sends a random draw to to process <em>i+1</em>. The process with the highest number will send to the root process. Notice that we are communicating in a ring. For communication, only use <code class="docutils literal notranslate"><span class="pre">Send</span></code> and <code class="docutils literal notranslate"><span class="pre">Recv</span></code>. THe program should work for any number of processes. (Hint: Remember that <code class="docutils literal notranslate"><span class="pre">Send</span></code> and <code class="docutils literal notranslate"><span class="pre">Recv</span></code> are <em>blocking</em> functions. Because each process will be sending and receiving, if every process calls its <code class="docutils literal notranslate"><span class="pre">Recv</span></code> first, the program will hang. What about if each calls <code class="docutils literal notranslate"><span class="pre">Send</span></code>?)</p></li>
<li><p>Implement the <em>Load-balancing</em> fix to the code <em>trapParallel_1.py</em>. The program should be able to take in any number of trapezoids <em>n</em> for any number of processes and the trapezoids should divide themselves among the processes evenly, differing by at most one between any two processes.</p></li>
</ol>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Point-to-Point Communication</a><ul>
<li><a class="reference internal" href="#send-and-recv">Send(…) and Recv(…)</a></li>
<li><a class="reference internal" href="#the-trapezoidal-rule">The Trapezoidal Rule</a></li>
<li><a class="reference internal" href="#parallelizing-the-trapezoidal-rule">Parallelizing the Trapezoidal Rule</a></li>
<li><a class="reference internal" href="#load-balancing">Load Balancing</a></li>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#exercises">Exercises</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="introMPI.html"
                        title="previous chapter">Introduction to MPI</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="collectiveCom.html"
                        title="next chapter">Collective Communication</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/pointToPoint.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="collectiveCom.html" title="Collective Communication"
             >next</a> |</li>
        <li class="right" >
          <a href="introMPI.html" title="Introduction to MPI"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">A Python Introduction to Parallel Programming with MPI 1.0.2 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2013, Jeremy Bejarano.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.0.1.
    </div>
  </body>
</html>