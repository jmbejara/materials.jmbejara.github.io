
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Collective Communication &#8212; A Python Introduction to Parallel Programming with MPI 1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="I/O, Debugging, and Performance" href="IOandDebugging.html" />
    <link rel="prev" title="Point-to-Point Communication" href="pointToPoint.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="collective-communication">
<span id="collectivecom"></span><h1>Collective Communication<a class="headerlink" href="#collective-communication" title="Permalink to this headline">¶</a></h1>
<p>In <a class="reference internal" href="pointToPoint.html#pointtopoint"><span class="std std-ref">Point-to-Point Communication</span></a>, we encountered a bottle neck in our trapezoidal rule program. It arose because we were requiring one process to sum the results of all the other processes. In the case of the code presented in the previous chapter, the root process <em>0</em> did all the work of summing the results while the other processes idled. A more efficient algorithm would balance the work-load so that there are non-idling processes. This problem can be somewhat corrected through what is called “Collective Communication.” Besides helping to alleviate load imbalances, however, collective communication has a more important purpose: it helps to optimize message passing among separate processes. Communication among processes is expensive. Because each message must be sent over some sort of network, we must minimize and optimize this inter-process communication. In this chapter we show how this can be accomplished in-part with collective communication.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are two important principles to remember here:</p>
<ul class="simple">
<li><p><strong>Load Balancing:</strong> A program is inefficient if it is not using all of the available resources (e.g., processes are idling because they are waiting on another)</p></li>
<li><p><strong>Communication is Expensive:</strong> <em>Broadcast</em> and <em>Reduce</em> are designed to optimize communication among the whole communicator. However, any sort of message passing is extremely expensive and one of the main obstacles to obtaining speedups when parallelizing an algorithm.</p></li>
</ul>
</div>
<div class="section" id="tree-structured-computation">
<h2>Tree-Structured Computation<a class="headerlink" href="#tree-structured-computation" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have eight processes, each with a number to be summed. Let the even ranked processes send their data to the odd process one greater than each respective process. The odd processes receive data from directly below them in rank. By so doing, we have in one time-step done half of the work.</p>
<div class="figure align-center" id="id1">
<span id="fastsum"></span><img alt="_images/fastSum.png" src="_images/fastSum.png" />
<p class="caption"><span class="caption-text">Fast Summation</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>From this point we can repeat the process with the odd processes, further partitioning them. A summation done in this manner creates a tree structure (see <a class="reference internal" href="#fastsum"><span class="std std-ref">Fast Summation</span></a>). MPI has implemented fast summations and much more in methods referred to as “Collective Communication.” The method shown in the figure is called <em>Reduce</em>.</p>
<p>Furthermore, MPI has methods that can also distribute information in efficient ways. Imagine reversing the arrows in the figure <a class="reference internal" href="#fastsum"><span class="std std-ref">Fast Summation</span></a>. This is MPI’s <em>Broadcast</em> function. We should note, though, that the image is a simplification of how MPI performs collective communication. In practice, it is different in every implementation, and is usually more efficient than the simplified example shown. MPI is designed to optimize these methods under the hood, and so we only need to worry about properly applying these functions.</p>
<p>Knowing this, let’s look at the revised trapezoid rule.</p>
</div>
<div class="section" id="the-parallel-trapezoidal-rule-2-0">
<h2>The Parallel Trapezoidal Rule 2.0<a class="headerlink" href="#the-parallel-trapezoidal-rule-2-0" title="Permalink to this headline">¶</a></h2>
<p>Below is the code for our revised edition of the trapezoid rule. Highlighted in bold is the main change that occurred between the two versions. The statement <code class="docutils literal notranslate"><span class="pre">comm.Reduce(integral,</span> <span class="pre">total)</span></code> has replaced the entire if else statement, including the for loop. Not only does the code look cleaner, it runs faster. The first argument, <code class="docutils literal notranslate"><span class="pre">integral</span></code>, is the number which is going to be added up, and which is different for every process. The variable <code class="docutils literal notranslate"><span class="pre">total</span></code> will hold the result:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#trapParallel_2.py</span>
<span class="c1">#example to run: mpiexec -n 4 python26 trapParallel_2.py 0.0 1.0 10000</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="kn">from</span> <span class="nn">mpi4py.MPI</span> <span class="kn">import</span> <span class="n">ANY_SOURCE</span>

<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_size</span><span class="p">()</span>

<span class="c1">#takes in command-line arguments [a,b,n]</span>
<span class="n">a</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

<span class="c1">#we arbitrarily define a function to integrate</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span>

<span class="c1">#this is the serial version of the trapezoidal rule</span>
<span class="c1">#parallelization occurs by dividing the range among processes</span>
<span class="k">def</span> <span class="nf">integrateRange</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">integral</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">b</span><span class="p">))</span><span class="o">/</span><span class="mf">2.0</span>
        <span class="c1"># n+1 endpoints, but n trapazoids</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                        <span class="n">integral</span> <span class="o">=</span> <span class="n">integral</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">integral</span> <span class="o">=</span> <span class="n">integral</span><span class="o">*</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="k">return</span> <span class="n">integral</span>


<span class="c1">#h is the step size. n is the total number of trapezoids</span>
<span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
<span class="c1">#local_n is the number of trapezoids each process will calculate</span>
<span class="c1">#note that size must divide n</span>
<span class="n">local_n</span> <span class="o">=</span> <span class="n">n</span><span class="o">/</span><span class="n">size</span>

<span class="c1">#we calculate the interval that each process handles</span>
<span class="c1">#local_a is the starting point and local_b is the endpoint</span>
<span class="n">local_a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">rank</span><span class="o">*</span><span class="n">local_n</span><span class="o">*</span><span class="n">h</span>
<span class="n">local_b</span> <span class="o">=</span> <span class="n">local_a</span> <span class="o">+</span> <span class="n">local_n</span><span class="o">*</span><span class="n">h</span>

<span class="c1">#initializing variables. mpi4py requires that we pass numpy objects.</span>
<span class="n">integral</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># perform local computation. Each process integrates its own interval</span>
<span class="n">integral</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">integrateRange</span><span class="p">(</span><span class="n">local_a</span><span class="p">,</span> <span class="n">local_b</span><span class="p">,</span> <span class="n">local_n</span><span class="p">)</span>

<span class="c1"># communication</span>
<span class="c1"># root node receives results with a collective &quot;reduce&quot;</span>
<span class="hll"><span class="n">comm</span><span class="o">.</span><span class="n">Reduce</span><span class="p">(</span><span class="n">integral</span><span class="p">,</span> <span class="n">total</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">MPI</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span>
<span class="c1"># root process prints results</span>
<span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;With n =&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="s2">&quot;trapezoids, our estimate of the integral from&quot;</span>\
        <span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="s2">&quot;to&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="n">total</span>
</pre></div>
</div>
<p>The careful observer may have noticed from the figure :ref:fastSum that in the end of the call to <code class="docutils literal notranslate"><span class="pre">Reduce</span></code>, only the root process holds the final summation. This brings up an important point: <em>collective communication calls can return different values to each process</em>. This can be a “gotcha” if you are not paying attention.</p>
<p>So what can we do if we want to return the same result for a summation to all processes: use the provided subroutine <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>. It does the same thing as <code class="docutils literal notranslate"><span class="pre">Reduce</span></code>, but it simultaneously uses each process as the root. That way, by the end of the summation, each process has calculated an identical sum. Duplicate work was done, but in the end, the result ends up on each process at the same time.</p>
<p>So when should we use <code class="docutils literal notranslate"><span class="pre">Reduce</span></code>, over <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>? If it is unimportant for all processes to have the result, as is the case with the Trapezoidal Rule, using Reduce should be slightly faster than using <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>. On the other hand, if every process needs the information, <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> is the fastest way to go. We could come up with some other solutions to the situation, such as a call to Reduce, followed by a call to Broadcast, however these other solutions are always less efficient than just using one call to the collective communication routines. In the end, the less communications, the better.</p>
<p>Theoretically, Allreduce could be just as fast as Reduce. Insert Butterfly communication pattern here. TODO</p>
</div>
<div class="section" id="reduce-and-allreduce">
<h2>Reduce(…) and Allreduce(…)<a class="headerlink" href="#reduce-and-allreduce" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="Comm.Reduce">
<code class="descclassname">Comm.</code><code class="descname">Reduce</code><span class="sig-paren">(</span><em>sendbuf</em>, <em>recvbuf</em>, <em>Op op = MPI.SUM</em>, <em>root = 0</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduces values on all processes to a single value onto the <em>root</em> process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Comm</strong> (<em>MPI comm</em>) – communicator we wish to query</p></li>
<li><p><strong>sendbuf</strong> (<em>choice</em>) – address of send buffer</p></li>
<li><p><strong>recvbuf</strong> (<em>choice</em>) – address of receive buffer (only significant at root)</p></li>
<li><p><strong>op</strong> (<em>handle</em>) – reduce operation</p></li>
<li><p><strong>root</strong> (<em>int</em>) – rank of root operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="k">import</span> <span class="n">MPI</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>

<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>
<span class="n">rankF</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Reduce</span><span class="p">(</span><span class="n">rankF</span><span class="p">,</span><span class="n">total</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">MPI</span><span class="o">.</span><span class="n">MAX</span><span class="p">)</span>
</pre></div>
</div>
<dl class="function">
<dt id="Comm.Allreduce">
<code class="descclassname">Comm.</code><code class="descname">Allreduce</code><span class="sig-paren">(</span><em>sendbuf</em>, <em>recvbuf</em>, <em>Op op = MPI.SUM</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Allreduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduces values on all processes to a single value onto all processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Comm</strong> (<em>MPI comm</em>) – communicator we wish to query</p></li>
<li><p><strong>sendbuf</strong> (<em>choice</em>) – starting address of send buffer</p></li>
<li><p><strong>recvbuf</strong> (<em>choice</em>) – starting address of receive buffer</p></li>
<li><p><strong>op</strong> (<em>handle</em>) – operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<blockquote>
<div><p>Same as the example for <em>Reduce</em> except replace <code class="docutils literal notranslate"><span class="pre">comm.Reduce</span></code> with <code class="docutils literal notranslate"><span class="pre">comm.Allreduce</span></code>. Notice that all process now have the “reduced” value.</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Within the <em>Op</em> class are values that represent predefine operations to be used with the “reduce” functions. There are also methods that allow the creation of user-defined operations. I present here a table of the predefined operations. Full documentation is found here: <a class="reference external" href="http://mpi4py.scipy.org/docs/apiref/mpi4py.MPI.Op-class.html">http://mpi4py.scipy.org/docs/apiref/mpi4py.MPI.Op-class.html</a></p>
</div>
<div class="section" id="the-op-class-reduction-operators">
<h3>The <em>Op</em> Class (Reduction Operators)<a class="headerlink" href="#the-op-class-reduction-operators" title="Permalink to this headline">¶</a></h3>
<p>These are the predefined operations that can be used for the input parameters <em>Op</em> (usage: <code class="docutils literal notranslate"><span class="pre">comm.Reduce(sendbuf,recvbuf,</span> <span class="pre">op=MPI.PROD)</span></code>). For more informaiton, see the section in the appendix, <a class="reference internal" href="appendix.html#opclass"><span class="std std-ref">The Op Class (Reduction Operations)</span></a>.</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 36%" />
<col style="width: 64%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MPI.MAX</p></td>
<td><p>maximum</p></td>
</tr>
<tr class="row-odd"><td><p>MPI.MIN</p></td>
<td><p>minimum</p></td>
</tr>
<tr class="row-even"><td><p>MPI.SUM</p></td>
<td><p>sum</p></td>
</tr>
<tr class="row-odd"><td><p>MPI.PROD</p></td>
<td><p>product</p></td>
</tr>
<tr class="row-even"><td><p>MPI.LAND</p></td>
<td><p>logical and</p></td>
</tr>
<tr class="row-odd"><td><p>MPI.BAND</p></td>
<td><p>bit-wise and</p></td>
</tr>
<tr class="row-even"><td><p>MPI.LOR</p></td>
<td><p>logical or</p></td>
</tr>
<tr class="row-odd"><td><p>MPI.BOR</p></td>
<td><p>bit-wise or</p></td>
</tr>
<tr class="row-even"><td><p>MPI.LXOR</p></td>
<td><p>logical xor</p></td>
</tr>
<tr class="row-odd"><td><p>MPI.BXOR</p></td>
<td><p>bit-wise xor</p></td>
</tr>
<tr class="row-even"><td><p>MPI.MAXLOC</p></td>
<td><p>max value and location</p></td>
</tr>
<tr class="row-odd"><td><p>MPI.MINLOC</p></td>
<td><p>min value and location</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="dot-product">
<h2>Dot Product<a class="headerlink" href="#dot-product" title="Permalink to this headline">¶</a></h2>
<p>Calculating a dot product is a relatively simple task which does not need to be parallelized, but it makes a good example for introducing the other important collective communication subroutines. When provided two vectors, the dot product, is simply the sum of the element wise multiplications of the two vectors:</p>
<div class="math notranslate nohighlight">
\[\sum_{k=1}^{n}u_{k}v_{k}\]</div>
<div class="section" id="parallelizing-the-dot-product">
<h3>Parallelizing the Dot Product<a class="headerlink" href="#parallelizing-the-dot-product" title="Permalink to this headline">¶</a></h3>
<p>Speedups can be gained by parallelizing the previous operation. We can divide up the work among different processors by sending pieces of the original vectors to different processors. Each processor then multiplies its elements and sums them. Finally, the local sums are summed using <code class="docutils literal notranslate"><span class="pre">Comm.Reduce</span></code>, which sums numbers distributed among many processes in <em>O(log n)</em> time. The procedure is as follows:</p>
<ol class="arabic simple">
<li><p>Divide the n-length arrays among p processes.</p></li>
<li><p>Run the serial dot product on each process’s local data.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">Reduce</span></code> with <code class="docutils literal notranslate"><span class="pre">MPI_SUM</span></code> to collect the result.</p></li>
</ol>
<p>First, we must divide up the array. We will do this with the function <code class="docutils literal notranslate"><span class="pre">Scatter(...)</span></code>. THen, we must write the code that implements the serial operation. For this, we just call numpy’s implementation: numpy.dot(a, b). We conlude with <code class="docutils literal notranslate"><span class="pre">Reduce(...)</span></code>.</p>
<p>The code looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#dotProductParallel_1.py</span>
<span class="c1">#&quot;to run&quot; syntax example: mpiexec -n 4 python26 dotProductParallel_1.py 40000</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_size</span><span class="p">()</span>

<span class="c1">#read from command line</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>    <span class="c1">#length of vectors</span>

<span class="c1">#arbitrary example vectors, generated to be evenly divided by the number of</span>
<span class="c1">#processes for convenience</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span>

<span class="c1">#initialize as numpy arrays</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
<span class="n">local_n</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

<span class="c1">#test for conformability</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
                                <span class="k">print</span> <span class="s2">&quot;vector length mismatch&quot;</span>
                                <span class="n">comm</span><span class="o">.</span><span class="n">Abort</span><span class="p">()</span>

                <span class="c1">#currently, our program cannot handle sizes that are not evenly divided by</span>
                <span class="c1">#the number of processors</span>
                <span class="k">if</span><span class="p">(</span><span class="n">n</span> <span class="o">%</span> <span class="n">size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
                                <span class="k">print</span> <span class="s2">&quot;the number of processors must evenly divide n.&quot;</span>
                                <span class="n">comm</span><span class="o">.</span><span class="n">Abort</span><span class="p">()</span>

                <span class="c1">#length of each process&#39;s portion of the original vector</span>
                <span class="n">local_n</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">n</span><span class="o">/</span><span class="n">size</span><span class="p">])</span>

<span class="c1">#communicate local array size to all processes</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Bcast</span><span class="p">(</span><span class="n">local_n</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="hll">
</span><span class="c1">#initialize as numpy arrays</span>
<span class="n">local_x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">local_n</span><span class="p">)</span>
<span class="n">local_y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">local_n</span><span class="p">)</span>

<span class="c1">#divide up vectors</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">local_x</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="hll"><span class="n">comm</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">local_y</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="hll">
</span><span class="c1">#local computation of dot product</span>
<span class="n">local_dot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">local_x</span><span class="p">,</span> <span class="n">local_y</span><span class="p">)])</span>

<span class="c1">#sum the results of each</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Reduce</span><span class="p">(</span><span class="n">local_dot</span><span class="p">,</span> <span class="n">dot</span><span class="p">,</span> <span class="n">op</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
<span class="hll">
</span><span class="k">if</span> <span class="p">(</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">print</span> <span class="s2">&quot;The dot product is&quot;</span><span class="p">,</span> <span class="n">dot</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;computed in parallel&quot;</span>
                <span class="k">print</span> <span class="s2">&quot;and&quot;</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="s2">&quot;computed serially&quot;</span>
</pre></div>
</div>
<p>The important parts of this algorithm are the collective communications, highlighted in the source code. The first call, which is to <code class="docutils literal notranslate"><span class="pre">Bcast(...)</span></code>, simply tells the other processors how big of an array to allocate for their portion of the original vectors: they will each allocate n/size slots for x and for y.</p>
<p>The actual distributing of the data occurs with the calls to <code class="docutils literal notranslate"><span class="pre">Scatter(...)</span></code>. <code class="docutils literal notranslate"><span class="pre">Scatter</span></code> will take an array and divide it up over all the processes. From this point, the serial dot product is run to calculate a local dot product. Now the only thing left to do is to add up the partial sums.</p>
<p>Reduce is the final part of the solution: it is called on line 7, and when it is finished, process 0 will contain the total dot product.</p>
<p>With this new algorithm and enough processors, the runtime approaches O(log n) time. To help understand how much faster this is compared to the original version, imagine that we have as many processors as we have items in each array: say, 1000. Then, each of these operations requires the same amount of time, the algorithm’s runtime would run hypothetically something like this:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 27%" />
<col style="width: 21%" />
<col style="width: 27%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task vs Time</p></th>
<th class="head"><p>Serial Code</p></th>
<th class="head"><p>Point-to-Point mpi</p></th>
<th class="head"><p>Collective mpi</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Broadcast n = 1000</p></td>
<td><p>N/A</p></td>
<td><p>About 1000</p></td>
<td><p>About 10</p></td>
</tr>
<tr class="row-odd"><td><p>Scatter x</p></td>
<td><p>N/A</p></td>
<td><p>About 1000</p></td>
<td><p>About 10</p></td>
</tr>
<tr class="row-even"><td><p>Scatter y</p></td>
<td><p>N/A</p></td>
<td><p>About 1000</p></td>
<td><p>About 10</p></td>
</tr>
<tr class="row-odd"><td><p>Compute sum</p></td>
<td><p>1000</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>Reduce result</p></td>
<td><p>N/A</p></td>
<td><p>About 1000</p></td>
<td><p>About 10</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Total</strong></p></td>
<td><p><strong>About 1000</strong></p></td>
<td><p><strong>About 4001</strong></p></td>
<td><p><strong>About 41</strong></p></td>
</tr>
</tbody>
</table>
<p>The first column shows the serial dot product’s time requirements. The final column shows the mpi-version. It is clear that the final version requires less time and that this number drops with the number of processors used. We simply cannot get this speed up unless we break out of the serial paradigm.</p>
<p>The middle column illustrates why we should use <em>broadcast</em>, and <em>scatter</em>, and <em>reduce</em> over using <em>n</em> <em>send/recv</em> pairs in a for-loop. We do not include code for this variation. <em>It turns out that using MPI in this fashion can actually run slower than the serial implementation of our dot product program.</em></p>
</div>
</div>
<div class="section" id="bcast-and-scatter">
<h2>Bcast(…) and Scatter(…)<a class="headerlink" href="#bcast-and-scatter" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="Comm.Bcast">
<code class="descclassname">Comm.</code><code class="descname">Bcast</code><span class="sig-paren">(</span><em>buf</em>, <em>root=0</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Bcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a message from the process with rank “root” to all other processes of the group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Comm</strong> (<em>MPI comm</em>) – communicator across which to broadcast</p></li>
<li><p><strong>buf</strong> (<em>choice</em>) – buffer</p></li>
<li><p><strong>root</strong> (<em>int</em>) – rank of root operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="k">import</span> <span class="n">MPI</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>

<span class="c1">#intialize</span>
<span class="n">rand_num</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">rand_num</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">comm</span><span class="o">.</span><span class="n">Bcast</span><span class="p">(</span><span class="n">rand_num</span><span class="p">,</span> <span class="n">root</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span> <span class="s2">&quot;Process&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;has the number&quot;</span><span class="p">,</span> <span class="n">rand_num</span>
</pre></div>
</div>
<dl class="function">
<dt id="Comm.Scatter">
<code class="descclassname">Comm.</code><code class="descname">Scatter</code><span class="sig-paren">(</span><em>sendbuf</em>, <em>recvbuf</em>, <em>root</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Sends data from one process to all other processes in a communicator (Length of recvbuf must evely divide the length of sendbuf. Otherwise, use <em>Scatterv</em>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sendbuf</strong> (<em>choice</em>) – address of send buffer (significant only at root)</p></li>
<li><p><strong>recvbuf</strong> (<em>choice</em>) – address of receive buffer</p></li>
<li><p><strong>root</strong> (<em>int</em>) – rank of sending process</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="k">import</span> <span class="n">MPI</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_size</span><span class="p">()</span>
<span class="n">LENGTH</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1">#create vector to divide</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1">#the size is determined so that length of recvbuf evenly divides the</span>
        <span class="c1">#length of sendbuf</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="o">*</span><span class="n">LENGTH</span><span class="p">,</span><span class="n">size</span><span class="o">*</span><span class="n">LENGTH</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
        <span class="c1">#all processes must have a value for x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1">#initialize as numpy array</span>
<span class="n">x_local</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">)</span>

<span class="c1">#all processes must have a value for x. But only the root process</span>
<span class="c1">#is relevant. Here, all other processes have x = None.</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_local</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">#you should notice that only the root process has a value for x that</span>
<span class="c1">#is not &quot;None&quot;</span>
<span class="nb">print</span> <span class="s2">&quot;process&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;x:&quot;</span><span class="p">,</span> <span class="n">x</span>
<span class="nb">print</span> <span class="s2">&quot;process&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;x_local:&quot;</span><span class="p">,</span> <span class="n">x_local</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The length of the <code class="docutils literal notranslate"><span class="pre">recvbuf</span></code> vector must evely divide the <code class="docutils literal notranslate"><span class="pre">sendbuf</span></code> vector. Otherwise, use <em>Scatterv</em>. (The same principle holds for the function <em>Gather</em> and <em>Gatherv</em> which undoes <em>Scatter</em> and <em>Scatterv</em>. See exercise TODO.</p>
</div>
</div>
<div class="section" id="a-closer-look-at-broadcast-and-reduce">
<h2>A Closer Look at Broadcast and Reduce<a class="headerlink" href="#a-closer-look-at-broadcast-and-reduce" title="Permalink to this headline">¶</a></h2>
<p>Let’s take a closer look at the call to <code class="docutils literal notranslate"><span class="pre">comm.Bcast</span></code>. <code class="docutils literal notranslate"><span class="pre">Bcast</span></code> sends data from one process to all others. It uses the same tree-structured communication illustrated in the Figure <a class="reference internal" href="#fastsum"><span class="std std-ref">Fast Summation</span></a>. It takes for its first argument an array of data, which must exist on all processors. However, what is contained in that array may be insignificant. The second argument tells Bcast which process has the useful information. It then proceeds to overwrite any data in the arrays of all other processes.</p>
<p>Bcast behaves as if it is synchronous. “Synchronous” means that all processes are in synch with each other, as if being controlled by a global clock tick. For example, if all processes make a call to <code class="docutils literal notranslate"><span class="pre">Bcast</span></code> they will all be guaranteed to be calling the subroutine at basically the same time.</p>
<p>For all practical purposes, you can also think of <code class="docutils literal notranslate"><span class="pre">Reduce</span></code> as being synchronous. The difference is that Reduce only has one receiving process, and that process is the only process whose data is guaranteed to contain the correct value at completion of the call. To test your understanding of Reduce, suppose we are adding the number one up by calling <code class="docutils literal notranslate"><span class="pre">Comm.Reduce</span></code> on 100 processes, with the root being process 0. The documentation of <code class="docutils literal notranslate"><span class="pre">Comm.Reduce</span></code> tells us that the receive buffer of process 0 will contain the number 100. What will be in the receive buffer of process 1?</p>
<p>The answer is we don’t know. The reduce could be implemented in one of several ways, dependent on several factors, and as a result it is non-deterministic. During collective communications such as <code class="docutils literal notranslate"><span class="pre">Reduce</span></code>, we have no guarantee of what value will be in the intermediate processes’ receive buffer. More importantly, future calculations should never rely on this data except in process root (process <em>0</em> in our case).</p>
</div>
<div class="section" id="scatterv-and-gatherv">
<h2>Scatterv(…) and Gatherv(…)<a class="headerlink" href="#scatterv-and-gatherv" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="Comm.Scatterv">
<code class="descclassname">Comm.</code><code class="descname">Scatterv</code><span class="sig-paren">(</span><span class="optional">[</span><em>choice sendbuf</em>, <em>tuple_int sendcounts</em>, <em>tuple_int displacements</em>, <em>MPI_Datatype sendtype</em>, <span class="optional">]</span><em>choice recvbuf</em>, <em>root=0</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Scatterv" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatter data from one process to all other processes in a group providing different amount of data and displacements at the sending side</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Comm</strong> (<em>MPI comm</em>) – communicator across which to scatter</p></li>
<li><p><strong>sendbuf</strong> (<em>choice</em>) – buffer</p></li>
<li><p><strong>sendcounts</strong> (<em>tuple_int</em>) – number of elements to send to each process (one integer for each process)</p></li>
<li><p><strong>displacements</strong> (<em>tuple_int</em>) – number of elements away from the first element in the array at which to begin the new, segmented array</p></li>
<li><p><strong>sendtype</strong> (<em>MPI_Datatype</em>) – MPI datatype of the buffer being sent (choice of sendbuf)</p></li>
<li><p><strong>recvbuf</strong> (<em>choice</em>) – buffer in which to receive the sendbuf</p></li>
<li><p><strong>root</strong> (<em>int</em>) – process from which to scatter</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># for correct performance, run unbuffered with 3 processes:</span>
<span class="c1"># mpiexec -n 3 python26 scratch.py -u</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="k">import</span> <span class="n">MPI</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">xlocal</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
        <span class="n">xlocal</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span> <span class="s2">&quot;Scatter&quot;</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Scatterv</span><span class="p">([</span><span class="n">x</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">MPI</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">],</span><span class="n">xlocal</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;process &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; has &quot;</span> <span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">xlocal</span><span class="p">))</span>

<span class="n">comm</span><span class="o">.</span><span class="n">Barrier</span><span class="p">()</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span> <span class="s2">&quot;Gather&quot;</span>
        <span class="n">xGathered</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
        <span class="n">xGathered</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Gatherv</span><span class="p">(</span><span class="n">xlocal</span><span class="p">,[</span><span class="n">xGathered</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">MPI</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;process &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; has &quot;</span> <span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">xGathered</span><span class="p">))</span>
</pre></div>
</div>
<dl class="function">
<dt id="Comm.Gatherv">
<code class="descclassname">Comm.</code><code class="descname">Gatherv</code><span class="sig-paren">(</span><em>choice sendbuf</em>, <span class="optional">[</span><em>choice recvbuf</em>, <em>tuple_int sendcounts</em>, <em>tuple_int displacements</em>, <em>MPI_Datatype sendtype</em>, <span class="optional">]</span><em>root=0</em><span class="sig-paren">)</span><a class="headerlink" href="#Comm.Gatherv" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather Vector, gather data to one process from all other processes in a group providing different amount of data and displacements at the receiving sides</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Comm</strong> (<em>MPI comm</em>) – communicator across which to scatter</p></li>
<li><p><strong>sendbuf</strong> (<em>choice</em>) – buffer</p></li>
<li><p><strong>recvbuf</strong> (<em>choice</em>) – buffer in which to receive the sendbuf</p></li>
<li><p><strong>sendcounts</strong> (<em>tuple_int</em>) – number of elements to receive from each process (one integer for each process)</p></li>
<li><p><strong>displacements</strong> (<em>tuple_int</em>) – number of elements away from the first element in the receiving array at which to begin appending the segmented array</p></li>
<li><p><strong>sendtype</strong> (<em>MPI_Datatype</em>) – MPI datatype of the buffer being sent (choice of sendbuf)</p></li>
<li><p><strong>root</strong> (<em>int</em>) – process from which to scatter</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<blockquote>
<div><p>See the example for Scatterv().</p>
</div></blockquote>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>What is the difference between <code class="docutils literal notranslate"><span class="pre">Reduce</span></code> and <code class="docutils literal notranslate"><span class="pre">Allreduce</span></code>?</p></li>
<li><p>Why is <code class="docutils literal notranslate"><span class="pre">Allreduce</span></code> faster than a <code class="docutils literal notranslate"><span class="pre">Reduce</span></code> followed by a <code class="docutils literal notranslate"><span class="pre">Bcast</span></code>?</p></li>
<li><p>In our parallel implementation of the calculation of the dot product, “dotProductParallel_1.py”, the number of processes must evenly divide the length of the vectors. Rewrite the code so that it runs regardless of vector length and number of processes (though for convenience, you may assume that the vector length is greater than the number of processes). Remember the principle of load balancing. Use Scatterv() to accomplish this.</p></li>
<li><p>Alter your code from the previous exercise so that it calculates the supremum norm(the maximal element) of one of the vectors (choose any one). This will include changing the operator Op in the call to <code class="docutils literal notranslate"><span class="pre">Reduce</span></code>.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">Scatter</span></code> to parallellize the multiplication of a matrix and a vector. There are two ways that this can be accomplished. Both use <code class="docutils literal notranslate"><span class="pre">Scatter</span></code> to distribute the matrix, but one uses <code class="docutils literal notranslate"><span class="pre">Bcast</span></code> to distribute the vector and <code class="docutils literal notranslate"><span class="pre">Gather</span></code> to finish while the other uses <code class="docutils literal notranslate"><span class="pre">Scatter</span></code> to segment the vector and finishes with <code class="docutils literal notranslate"><span class="pre">Reduce</span></code>. Outline how each would be done. Discuss which would be more efficient (hint: think about memory usage). Then, write the code for the better one. Generate an arbitrary matrix on the root node. You may assume that the number of processes is equal to the number of rows (columns) of a square matrix. Example code demonstrating scattering a matrix is  shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpi4py</span> <span class="k">import</span> <span class="n">MPI</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">],[</span><span class="mf">4.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">6.</span><span class="p">],[</span><span class="mf">7.</span><span class="p">,</span><span class="mf">8.</span><span class="p">,</span><span class="mf">9.</span><span class="p">]])</span>
<span class="n">local_a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">comm</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">local_a</span><span class="p">)</span>
<span class="nb">print</span> <span class="s2">&quot;process&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;has&quot;</span><span class="p">,</span> <span class="n">local_a</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">A Python Introduction to Parallel Programming with MPI</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">An Overview of Parallel Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="introMPI.html">Introduction to MPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="pointToPoint.html">Point-to-Point Communication</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Collective Communication</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tree-structured-computation">Tree-Structured Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-parallel-trapezoidal-rule-2-0">The Parallel Trapezoidal Rule 2.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reduce-and-allreduce">Reduce(…) and Allreduce(…)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dot-product">Dot Product</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bcast-and-scatter">Bcast(…) and Scatter(…)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#a-closer-look-at-broadcast-and-reduce">A Closer Look at Broadcast and Reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scatterv-and-gatherv">Scatterv(…) and Gatherv(…)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="IOandDebugging.html">I/O, Debugging, and Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="pointToPoint.html" title="previous chapter">Point-to-Point Communication</a></li>
      <li>Next: <a href="IOandDebugging.html" title="next chapter">I/O, Debugging, and Performance</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Jeremy Bejarano.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/collectiveCom.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>